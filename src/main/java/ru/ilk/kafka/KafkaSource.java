package ru.ilk.kafka;//package ru.ilk.kafka;
//
////import com.sun.org.apache.xalan.internal.xsltc.compiler.util.StringType;
//
//import org.apache.log4j.Logger;
//import org.apache.spark.sql.Dataset;
//import org.apache.spark.sql.Row;
//import org.apache.spark.sql.SparkSession;
//import org.apache.spark.sql.functions;
////import ru.ilk.creditcard.Schema;
//
//public class KafkaSource {
//
//    private static Logger logger = Logger.getLogger(KafkaSource.class.getName());
//
//    /* Read stream from Kafka using Structured Streaming */
//    public static Dataset<Row> readStream(SparkSession sparkSession) {
//        String startingOption = "startingOffsets";
//        String partitionsAndOffsets = "earliest";
//        logger.info("Reading from Kafka");
//        //logger.info("partitionsAndOffsets: " + partitionsAndOffsets);
//        Dataset<Row> data = sparkSession
//                .readStream()
//                .format("kafka")
//                .option("kafka.bootstrap.servers", (String) KafkaConfig.kafkaParams.get("bootstrap.servers"))
//                .option("subscribe", (String) KafkaConfig.kafkaParams.get("topic"))
//                .option("enable.auto.commit", (Boolean) KafkaConfig.kafkaParams.get("enable.auto.commit")) // Cannot be set to true in Spark Strucutured Streaming https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html#kafka-specific-configurations
//                .option("group.id", (Double) KafkaConfig.kafkaParams.get("group.id"))
//                //.option(startingOption, partitionsAndOffsets) //this only applies when a new query is started and that resuming will always pick up from where the query left off
//                .load();
//
//
//        Dataset<Row> data2 = data.withColumn(Schema.kafkaTransactionStructureName, // nested structure with our json
//                        functions.from_json(data.col("value")
//                                ,Schema.kafkaTransactionSchema));
//        return data2;
////        Dataset<TransactionKafka> df = data.withColumn(Schema.kafkaTransactionStructureName, // nested structure with our json
////                functions.from_json(data.col("value")
////                        ,Schema.kafkaTransactionSchema)).as(TransactionKafka);
//    }
//}
